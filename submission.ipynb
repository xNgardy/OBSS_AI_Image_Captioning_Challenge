{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLqZ2mLOzXzc"
      },
      "outputs": [],
      "source": [
        "!pip install qwen_vl_utils\n",
        "!pip install -U bitsandbytes\n",
        "!pip install trl==0.12.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset as HFDataset, load_dataset\n",
        "import zipfile\n",
        "import shutil"
      ],
      "metadata": {
        "id": "hWgsymjDzf8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Configuration -----------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.use_deterministic_algorithms(True, warn_only=False)\n",
        "\n",
        "# Additional environment variables for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "os.environ['TRANSFORMERS_SEED'] = str(SEED)\n",
        "os.environ['PYTORCH_SEED'] = str(SEED)\n",
        "os.environ['NUMPY_SEED'] = str(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "CmT5Tkwfzih3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Download and Extract Dataset from Hugging Face -----------------\n",
        "# Creating local data directory\n",
        "LOCAL_DATA_DIR = \"/content/obss_data\"\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Downloading the zip file\n",
        "zip_path = os.path.join(LOCAL_DATA_DIR, \"obss-intern-competition-2025.zip\")\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading zip file from Hugging Face...\")\n",
        "    import requests\n",
        "    url = \"https://huggingface.co/datasets/obss/ai-intern-challenge-2025/resolve/main/obss-intern-competition-2025.zip\"\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "    with open(zip_path, 'wb') as f:\n",
        "        downloaded = 0\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "            downloaded += len(chunk)\n",
        "            if total_size > 0:\n",
        "                percent = (downloaded / total_size) * 100\n",
        "                print(f\"\\rDownloading: {percent:.1f}%\", end='')\n",
        "    print(f\"\\nDownloaded zip file to {zip_path}\")\n",
        "\n",
        "# Extracting the zip file\n",
        "if not os.path.exists(os.path.join(LOCAL_DATA_DIR, \"train\")):\n",
        "    print(\"Extracting zip file...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(LOCAL_DATA_DIR)\n",
        "    print(f\"Extracted files to {LOCAL_DATA_DIR}\")\n",
        "\n",
        "    print(\"\\nExtracted contents:\")\n",
        "    for root, dirs, files in os.walk(LOCAL_DATA_DIR):\n",
        "        level = root.replace(LOCAL_DATA_DIR, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")"
      ],
      "metadata": {
        "id": "Gy5iMogCzjt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = os.path.join(LOCAL_DATA_DIR, \"train\", \"train\")\n",
        "TEST_DIR = os.path.join(LOCAL_DATA_DIR, \"test\", \"test\")\n",
        "TRAIN_CSV = os.path.join(LOCAL_DATA_DIR, \"train.csv\")\n",
        "TEST_CSV = os.path.join(LOCAL_DATA_DIR, \"test.csv\")\n",
        "\n",
        "SUBMISSION_CSV = os.path.join(LOCAL_DATA_DIR, \"submission_pelinsu_kaleli.csv\")\n",
        "\n",
        "OUTPUT_DIR = \"/content/qwen25vl_lora_captioning\"\n",
        "DATASET_CACHE = \"/content/train_dataset_cache\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Quantization settings\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bf16 for A100 training\n",
        ")"
      ],
      "metadata": {
        "id": "9gqU9CYpzlkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Model & Processor -----------------\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# Enabling gradient checkpointing in order to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # All attention projections\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "        \"embed_tokens\", \"lm_head\"                # Input/output embeddings\n",
        "    ], # Got the best training results with selecting these modules.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")"
      ],
      "metadata": {
        "id": "jz9czM8wzoGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Dataset Preparation -----------------\n",
        "# Check if cached dataset exists\n",
        "if os.path.exists(DATASET_CACHE):\n",
        "    dataset = HFDataset.load_from_disk(DATASET_CACHE)\n",
        "else:\n",
        "    df = pd.read_csv(TRAIN_CSV).dropna()\n",
        "\n",
        "    # Helper to build one record with proper format\n",
        "    def make_record(row):\n",
        "        img_path = os.path.join(IMAGE_DIR, f\"{row.image_id}.jpg\")\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        prompt = \"\"\"Generate a single-sentence, objective, and descriptive caption for the given image. Strive to be as comprehensive and detailed as possible, keeping it between 15 to 25 words. Your caption should focus on multiple significant visible elements: identify primary subjects (e.g., people, animals) and their actions or notable characteristics; describe key objects and their attributes; include specific brand names or clearly legible text from signs/labels; and mention the immediate setting or context if prominent. The caption must be in the present tense, maintain a neutral, factual tone (like a museum or news catalog entry), and avoid subjective opinions.\"\"\"\n",
        "\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": prompt}\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": row.caption}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # Parallelize image loading\n",
        "    records = []\n",
        "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as exe:\n",
        "        futures = [exe.submit(make_record, row) for row in df.itertuples(index=False)]\n",
        "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Preparing samples\"):\n",
        "            records.append(fut.result())\n",
        "\n",
        "    # Create HF Dataset\n",
        "    # IT TAKES AROUND 25 MINUTES AFTER PREPARING SAMPLES WITHOUT A PROGRESS BAR DO NOT WORRY !!!\n",
        "    dataset = HFDataset.from_list(records)\n",
        "    dataset.save_to_disk(DATASET_CACHE)"
      ],
      "metadata": {
        "id": "7Bq-wzTwzrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for SFTTrainer\n",
        "def collate_fn(examples):\n",
        "    texts = []\n",
        "    images = []\n",
        "\n",
        "    for example in examples:\n",
        "        text = processor.apply_chat_template(\n",
        "            example['messages'],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "        images.append(example['image'])\n",
        "\n",
        "    batch = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    # Create labels\n",
        "    labels = batch['input_ids'].clone()\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    # Mask the prompt part (everything before assistant's response)\n",
        "    for i, text in enumerate(texts):\n",
        "        assistant_start = text.find(\"assistant\") + len(\"assistant\")\n",
        "        tokens = processor.tokenizer.encode(text[:assistant_start], add_special_tokens=False)\n",
        "        labels[i, :len(tokens)] = -100\n",
        "\n",
        "    batch['labels'] = labels\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "x_rpW_vXzs1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map dataset to add text field for SFTTrainer\n",
        "dataset = dataset.map(\n",
        "    lambda ex: {\n",
        "        \"text\": processor.apply_chat_template(\n",
        "            ex[\"messages\"],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "09pJcHulzuET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Training Arguments -----------------\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=4e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    logging_steps=20,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"steps\",\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    dataset_text_field=\"text\",\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # More memory efficient\n",
        "    seed=SEED,  # Setting seeds for both trainer -\n",
        "    data_seed=SEED,  # and data sampling for reproducibility\n",
        ")\n",
        "\n",
        "# ----------------- Trainer -----------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    peft_config=peft_config\n",
        ")"
      ],
      "metadata": {
        "id": "lx202DKIzvUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ----------------- Training -----------------\n",
        "# IT TAKES AROUND *10 HOURS* ON A100 40 GB\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ap_1iXodzwln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "processor.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Model and processor saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "5-EqvT6Ozx2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Quick Test -----------------\n",
        "print(\"\\nTesting the fine-tuned model...\")\n",
        "\n",
        "test_df_for_quick_test = pd.read_csv(TRAIN_CSV)\n",
        "test_row = test_df_for_quick_test.iloc[5]\n",
        "test_image_path = os.path.join(IMAGE_DIR, f\"{test_row['image_id']}.jpg\")\n",
        "\n",
        "test_seed = SEED + test_row['image_id']\n",
        "torch.manual_seed(test_seed)\n",
        "torch.cuda.manual_seed_all(test_seed)\n",
        "\n",
        "test_img = Image.open(test_image_path).convert(\"RGB\")\n",
        "\n",
        "# Test prompt, same as training\n",
        "test_prompt = \"\"\"Generate a single-sentence, objective, and descriptive caption for the given image. Strive to be as comprehensive and detailed as possible, keeping it between 15 to 25 words. Your caption should focus on multiple significant visible elements: identify primary subjects (e.g., people, animals) and their actions or notable characteristics; describe key objects and their attributes; include specific brand names or clearly legible text from signs/labels; and mention the immediate setting or context if prominent. The caption must be in the present tense, maintain a neutral, factual tone (like a museum or news catalog entry), and avoid subjective opinions.\"\"\"\n",
        "\n",
        "test_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": test_prompt}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "test_text_input = processor.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "test_inputs = processor(\n",
        "    text=[test_text_input],\n",
        "    images=[test_img],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ").to(device)\n",
        "\n",
        "# Generate caption\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_gen_ids = model.generate(\n",
        "        **test_inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        temperature=None,\n",
        "    )\n",
        "    test_generated_tokens = test_gen_ids[0][test_inputs['input_ids'].shape[1]:]\n",
        "    test_response = processor.tokenizer.decode(test_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(f\"Image ID: {test_row['image_id']}\")\n",
        "print(f\"Original caption: {test_row['caption']}\")\n",
        "print(f\"Generated caption: {test_response}\")"
      ],
      "metadata": {
        "id": "3AEAf0iVzzGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- INFERENCE SECTION -----------------"
      ],
      "metadata": {
        "id": "_Ysevs0Lz0am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen_vl_utils\n",
        "!pip install -U bitsandbytes\n",
        "!pip install trl==0.12.0"
      ],
      "metadata": {
        "id": "gPWSLSOFz1OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "P_E-UmPqz2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Configuration -----------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.use_deterministic_algorithms(True, warn_only=False)\n",
        "\n",
        "# Additional environment variables for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "os.environ['TRANSFORMERS_SEED'] = str(SEED)\n",
        "os.environ['PYTORCH_SEED'] = str(SEED)\n",
        "os.environ['NUMPY_SEED'] = str(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to the dataset and images (using local directories)\n",
        "LOCAL_DATA_DIR = \"/content/obss_data\"\n",
        "IMAGE_DIR = os.path.join(LOCAL_DATA_DIR, \"train\", \"train\")\n",
        "TEST_DIR = os.path.join(LOCAL_DATA_DIR, \"test\", \"test\")\n",
        "TRAIN_CSV = os.path.join(LOCAL_DATA_DIR, \"train.csv\")\n",
        "TEST_CSV = os.path.join(LOCAL_DATA_DIR, \"test.csv\")\n",
        "SUBMISSION_CSV = os.path.join(LOCAL_DATA_DIR, \"submission_pelinsu_kaleli.csv\")\n",
        "OUTPUT_DIR = \"/content/qwen25vl_lora_captioning\"\n",
        "DATASET_CACHE = \"/content/train_dataset_cache\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "Cj1O07-rz3v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Loading on T4 for Inference -----------------\n",
        "\n",
        "bnb_config_t4 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16  # fp16 for T4\n",
        ")\n",
        "\n",
        "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config_t4,\n",
        "    torch_dtype=torch.float16  # T4 uses fp16\n",
        ")"
      ],
      "metadata": {
        "id": "YnDcML6Nz6eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading lora weights\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    OUTPUT_DIR,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "NGbrVYAIz7vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Inference -----------------\n",
        "# Below I am processing received images in chunks because I was getting Cuda OOM error every time I tried to do inference.\n",
        "# After many hours of debugging, cause of the problem was actually the 350th image.\n",
        "# Therefore I have added many safety features until understanding the problem, including Cuda OOM error checking and realized the problem afterwards.\n",
        "# However since it is still a very reliable way of doing inference I have decided to keep it this way.\n",
        "# Takes around 5-6 hours total, each chunk around 9 minutes.\n",
        "\n",
        "CHUNK_SIZE = 100\n",
        "OUTPUT_PATH = SUBMISSION_CSV\n",
        "\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Function to clean captions in a neat format\n",
        "def clean_caption(caption):\n",
        "    caption = caption.strip()\n",
        "\n",
        "    if caption and caption[-1] not in '.!?':\n",
        "        caption += '.'\n",
        "\n",
        "    if caption:\n",
        "        caption = caption[0].upper() + caption[1:]\n",
        "\n",
        "    return caption\n",
        "\n",
        "# Function to process a single chunk\n",
        "def process_chunk(chunk_df, start_idx):\n",
        "    chunk_results = []\n",
        "    model.eval()\n",
        "\n",
        "    for idx, row in enumerate(tqdm(chunk_df.itertuples(index=False),\n",
        "                                   total=len(chunk_df),\n",
        "                                   desc=f\"Processing chunk starting at {start_idx}\")):\n",
        "        try:\n",
        "            image_id = row.image_id\n",
        "            img_path = os.path.join(TEST_DIR, f\"{image_id}.jpg\")\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            # Got the best result with this prompt, a little bit changed from the training one, maybe could've gotten a better score if this was used in the training as well.\n",
        "            prompt = \"\"\"Generate a single-sentence, objective, and descriptive caption for the given image. Strive to be as comprehensive and detailed as possible, keeping it between 15 to 30 words. Your caption should focus on multiple significant visible elements: identify primary subjects (e.g., people, animals) and their actions or notable characteristics; describe key objects with their specific attributes and spatial positions; include specific brand names, model numbers, or clearly legible text from signs/labels and mention the immediate setting, background, or foreground context when it helps understand the composition. The caption must be in the present tense, maintain a neutral, factual tone (like a museum or news catalog entry), and avoid subjective opinions.\"\"\"\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": prompt}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            text_input = processor.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            inputs = processor(\n",
        "                text=[text_input],\n",
        "                images=[img],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                gen_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=False,\n",
        "                    temperature=None,\n",
        "                )\n",
        "                generated_tokens = gen_ids[0][inputs['input_ids'].shape[1]:]\n",
        "                caption = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "            caption = clean_caption(caption)\n",
        "            chunk_results.append({'image_id': image_id, 'caption': caption})\n",
        "\n",
        "            del inputs, gen_ids, generated_tokens, img\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing image {image_id}: {e}\")\n",
        "            chunk_results.append({'image_id': image_id, 'caption': 'An image showing various objects and scenes.'})\n",
        "\n",
        "        # Clear cache every image\n",
        "        if (idx + 1) % 1 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return chunk_results\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# Process in chunks\n",
        "num_chunks = (len(test_df) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "print(f\"Total images: {len(test_df)}\")\n",
        "print(f\"Processing in {num_chunks} chunks of {CHUNK_SIZE} images each\\n\")\n",
        "\n",
        "for chunk_idx in range(num_chunks):\n",
        "    start_idx = chunk_idx * CHUNK_SIZE\n",
        "    end_idx = min(start_idx + CHUNK_SIZE, len(test_df))\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Processing Chunk {chunk_idx + 1}/{num_chunks}\")\n",
        "    print(f\"Images {start_idx} to {end_idx - 1}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    chunk_df = test_df.iloc[start_idx:end_idx]\n",
        "\n",
        "    chunk_results = process_chunk(chunk_df, start_idx)\n",
        "    all_results.extend(chunk_results)\n",
        "\n",
        "    intermediate_df = pd.DataFrame(all_results)\n",
        "    intermediate_path = OUTPUT_PATH.replace('.csv', f'_intermediate_{end_idx}.csv')\n",
        "    intermediate_df.to_csv(intermediate_path, index=False)\n",
        "    print(f\"\\nSaved intermediate results to {intermediate_path}\")\n",
        "    print(f\"Total captions generated: {len(all_results)}\")\n",
        "\n",
        "    # Clear memory after chunk\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU memory after chunk: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    import time\n",
        "    if chunk_idx < num_chunks - 1:\n",
        "        print(\"\\nWaiting 5 seconds before next chunk...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "# Save final results\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"Processing complete!\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "final_df = pd.DataFrame(all_results)\n",
        "final_df.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"\\nFinal results saved to {OUTPUT_PATH}\")\n",
        "print(f\"Total captions generated: {len(all_results)}\")\n",
        "\n",
        "print(\"\\nCleaning up intermediate files...\")\n",
        "for chunk_idx in range(num_chunks):\n",
        "    end_idx = min((chunk_idx + 1) * CHUNK_SIZE, len(test_df))\n",
        "    intermediate_path = OUTPUT_PATH.replace('.csv', f'_intermediate_{end_idx}.csv')\n",
        "    if os.path.exists(intermediate_path):\n",
        "        os.remove(intermediate_path)\n",
        "        print(f\"Removed {intermediate_path}\")"
      ],
      "metadata": {
        "id": "Nvf6i0i2z8uh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}